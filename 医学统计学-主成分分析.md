---
title: 主成分分析（1）——理论部分
date: 2017-09-22 12:02:00
type: "tags"
tags:
  - 基础统计
  - 基础知识
categories: 
  - 医学统计学
mathjax: true
---

## 主成分分析简介及主要思想

主成分分析（principal components analysis，PCA），是由1901年被Pearson首先引入的，1933年由Hotelling作了进一步的发展，主成分分析是从多个数值变量（指标）之间的相互关系入手，利用降维的思想，将多个变量（指标）化为少数几个互不相关的综合变量（指标）的统计方法。

在医学研究中，为了客观、全面地分析问题，常要记录多个观察指标并考虑众多的影响因素，这样的数据虽然可以提供丰富的信息，但同时也使用数据的分析工作更趋复杂化。例如在儿童生长发育的评价中，收集到的数据包括每一个儿童的身高、体重、胸围、坐高、肺活量等十多个指标。怎样利用这类多指标的数据对每一个儿童的生长发育水平作出正常的评价？如果仅用其中任一指标来作评价，其结论显然是片面的，而且不能充分利用已有的数据信息。如果分别利用每一指标进行评价，然后再综合各指标评价的结论，这样做一是可能会出现各指标评价的结论不一致，甚至相互冲突，从而给最后的综合评价带来困难；二是工作量明显增大，不利于进一步的统计分析。事实上，在实际工作中，所涉及的众多指标之间经常是有相互联系和影响的，从这一点出发，希望通过对原始相互关系的研究，找出少数几个综合指标，这些综合指标是原始指标的纯属绵且，它既保留了原始指标的主要信息，且又互不相关。这样一种从众多原始指标之间相互关系入手，寻找少数综合指标以根据原始指标信息的多元统计方法称为主成分分析。

## 主成分分析的数学模型及几何意义
###（一）主成分分析的数学模型
设有m个指标，即$X_{1},X_{2},\dots,X_{m}$,欲寻找可以概括这m个指标主要信息的综合指标$Z_{1},Z+{2},\dots,Z_{m}$，从数学上讲，就是寻找一组常数，即$a_{i1},a_{i2},\dots,a_{im}(i=1,2,\dots,m）$，使这m个指标的线性组合为：

$$
\left\{
\begin{array}{c}
Z_{1} =a_{11}X_{1}+a_{12}X_{2}+\dots+a_{1m}X_{m}\\
Z_{2}=a_{21}X_{1}+a_{2}X_{2}+\dots+a_{2m}X_{m}\\
  \vdots \\
Z_{m}=a_{m1}X_{1}+a_{m2}X_{2}+\dots+a_{mm}X_{m}
\end{array}
\right.(1-1)
$$

能够根据m个原始指标$X_{1},X_{2},\dots,X_{m}$的主要信息，且各$Z_{i}(i=1,2,\dots,m)$互不相关，为叙述方案，可以引入以下的矩阵形式，令

$$
{\begin{matrix}
{\bf Z}=\begin{pmatrix}
Z_{1}\\ 
Z_{2}\\ 
\vdots\\
Z_{m}
\end{pmatrix}, & {\bf A}=\begin{pmatrix}
a_{11} &a_{12}& \dots & a_{1m}\\ 
a_{21} &a_{22}& \dots & a_{2m}\\ 
\vdots & \vdots & \vdots & \vdots\\
a_{m1} &a_{m2}& \dots & a_{mm}\\ 
\end{pmatrix} &\overset{\Delta}{=}\begin{pmatrix}
a'_{1}\\ 
a'_{2}\\ 
\vdots\\
a'_{m}
\end{pmatrix},&{\bf X}=\begin{pmatrix}
X_{1}\\ 
X_{2}\\ 
\vdots\\
X_{m}
\end{pmatrix}
\end{matrix}},(1-2)
$$

则公式1-1可以表示为：
$$
\bf {Z=AX} (1-2)
$$
或者是
$$
\left\{
\begin{array}{c}
Z_{1} =\bf a'_{1}X\\
Z_{2}=\bf a'_{2}X\\\\
  \vdots \\
Z_{m}=\bf a'_{m}X\\
\end{array}
\right.
$$
如果$Z_{1}=\bf a'_{1}X$满足$\bf a'_{1}a_{1}=1$，且
$Var(Z_{1})=\mathop{Max}\limits_{\bf a'a=1}^{}{Var(\bf a'X)}$，刚称$Z_{1}$是原始指标$X_{1},X_{2},\dots,X_{m}$的第一主成分。

通常情况下，如果$Z_{i}=\bf a'X$满足：
(1)$\bf a'_{i}a_{i}=1$，当i>1时，$\bf a'_{i}a_{j}=0(i=1,2,\dots,i-1)$
(2)$Var(Z_{i})=\mathop{Max{Var(a'\bf X)}}\limits_{\bf a'a=1,\bf a'a_{j}=0(j=1,2,\dots,i-1)}^{}$
刚称$Z_{i}$是原始指标的第$i$主成分（$i=2,\dots,m$）。

由上述定义可知，当$i\neq j$时，主成分$Z_{i}$与$Z_{j}$是互不相关的，并且$Z_{1}$是原始指标$X_{1},X_{2},\dots,X_{m}$的一切线性组合中方差最大者，$Z_{2}$是与$Z_{1}$不相关的、除$Z_{1}$以外的$X_{1},X_{2},\dots,X_{m}$一切线性组合中方差最大者，$Z_{m}$是与$Z_{1},Z_{2},\dots,Z_{m}-1$都不相关的，除$Z_{1},Z_{2},\dots,Z_{m}-1$以外的$X_{1},X_{2},\dots,X_{m}$一切线性组合中方差最大者。从理论上讲，求得的主成分个数最多可能有m个，这时，m个主成分就反映了全部原始指标所提供的信息，鉴于主成分分析的目的主要是用较少的综合指标来反映全部原始指标中的主要信息，因此在实际工作中，所确定的主成分个数总是小于原始指标的个数。


### （二）主成分的几何意义
为方便讨论，以m=2为例说明主成分分析的几何意义，设个体具有两个观测指标X1和X2，它们之间具有较强的相关性，测量n例这样的个体的值，将所得的n对数据在以X1为横轴，X2为纵轴的二维坐标平面中的苫，得到如下的散点图：
![](http://ortxpw68f.bkt.clouddn.com/pca.png)

由上图可以看出，由于$X_{1}$和$X_{2}$具有较强的相关性，这n个点的分页呈现出直接化的趋势；同时它们沿$X_{1}$轴方向和$X_{2}$轴方向都具有较大的变异变。个体在某个方向上的变异度可以用该方向上相应观测变量的方差来定量地表示。显然，如果只考虑$X_{1}$和$X_{2}$中任何一个方向上的言状，就将损失原始观测数据中很大一部分信息。如果我们将坐标轴$X_{1}$和$X_{2}$同时按逆时针方向作一个放置，得到新的坐标轴$Z_{1}$和$Z_{2}$，使得在亲折坐标平面上，这n个点的分布基本上不再具有有相关性，且它们的变异主要集中在Z1方向上，而在Z2方向上的变异较小，此时若取$Z_{1}$作为第一主成分，则$Z_{1}$就反应了原始指标$X_{1}$和$X_{2}$所包含的主要信息。

## 主成分的求法及性质
### （一）主成分的求法
由主成分的定义可知，各主成分互不相关，即任意两个主成分$Z_{i},Z_{j}$的协方差为0，即
$$
Cov(Z_{i},Z_{j})=0,i\neq j
$$

且各主成分的方差满足：
$$
Var(Z_{1}) \geq Var(Z_{2}) \dots Var(Z_{m})  
$$

于是由公式(1-2)定义的随机向量$Z$的协方差矩阵为：
$$
{\begin{matrix}
{Cov({\bf Z})=Cov(\bf AX)={\bf A'} Cov({\bf X}){\bf A}}=\begin{pmatrix}
Var(Z_{1})  &  &  &  0\\ 
&  Var(Z_{2})&  &\\ 
&  &\ddots  &  \\ 
0  &  &  &  Var(Z_{m})
\end{pmatrix}
\end{matrix}}
$$

由主成分定义中的条件（1）可知，这里的方阵$A$是正交阵，即$A'A=I$（I为单位矩阵），由此可得：
$$
{\begin{matrix}
{Cov({\bf X}){\bf A}}=A\begin{pmatrix}
Var(Z_{1})  &  &  &  0\\ 
  &  Var(Z_{2})&  &\\ 
      &  &\ddots  &  \\ 
0  &  &  &  Var(Z_{m})
\end{pmatrix}
\end{matrix}}
(1-3)$$

由上述公式可知，求原始指标$X_{1},X_{2},\dots,X_{m}$的主成分问题，实际上就是要求满足上述条件的正交阵$A$，即随机微量$X=(X_{1},X_{2},\dots,X_{m}$的协方差矩阵$Cov(X)$的特征值（eigenvalue）与特征向量(eigenvector)。

### 主成分的计算过程
下面讨论怎么由一组$X_{1},X_{2},\dots,X_{m}$的样本观测值求出主成分，假设收集到的原始数据共有n例，每例测得m个指标的数值，记录如下所示：
![](http://ortxpw68f.bkt.clouddn.com/pca003.png)

#### 1. 对各原始指标数据进行标准化，先按下式进行：

$$
X'_{ij}=\frac{X_{ij}-\bar{X_{j}}}{S_{J}}(j=1,2,3,\dots,m)
$$

将原始指标标准化，然后用标准化的数据$X'_{ij}$来计算主成分，为了方便计算，下面的公式中仍用$X_{ij}$表示标准化后的指标数据，$\bf X$为标准化后的数据矩阵，则：

$$
{\begin{matrix}
{{\bf X}}=A\begin{pmatrix}
X_{11}&  X_{12} &  \dots  &X_{1m}  &  \\ 
X_{21}&  X_{22} &  \dots  &X_{2m}  &  \\ 
\vdots &  \vdots  &  \ddots &  \vdots  \\ 
X_{n1}&  X_{n2} &  \dots  &X_{nm}
\end{pmatrix}
\end{matrix}}
$$

#### 2. 求出$X$的相关矩阵$R$
标准化后，$X$的相关矩阵即为协方差矩阵$Cov({\bf X})$

$$
{\begin{matrix}
& {{\bf R} =Cov({\bf X})}=\begin{pmatrix}
r_{11} &r_{12}& \dots & r_{1m}\\ 
r_{21} &r_{22}& \dots & r_{2m}\\ 
\vdots & \vdots & \vdots & \vdots\\
r_{m1} &r_{m2}& \dots & r_{mm}\\ 
\end{pmatrix} & =\begin{pmatrix}
1 &r_{12}& \dots & r_{1m}\\ 
r_{21} & 1 & \dots & r_{2m}\\ 
\vdots & \vdots & \vdots & \vdots\\
r_{m1} &r_{m2}& \dots & 1 \\ 
\end{pmatrix} 
\end{matrix}}
$$

#### 3. 求出相关矩阵的特征值和特征值对应的特征向量
由公式1-3得知，求主成分的问题，实际上是求出$X$的协方差矩阵$Cov(X)$（这里即为$X$的相关矩阵$R$）的特征值和特征向量，由于$R$为半正定矩阵，故可由R的特征方程
$$
|{\bf R} -\lambda{\bf I} 
= 0
$$

解得每一特征值$\lambda_{i}$对应的单位特征向量$a_{i}=(a_{i1} a_{i2} \dots a_{im}'$，从而求得各主成分，即
$$
Z_{i}={\bf a'_{i}X}={a_{i1}X_{1}+a_{i2}X_{2}+ \dots +a_{im}X_{m}} i=1,2,\dots,m
$$

### （二）主成分的性质

#### 1. 各主成分互不相关

即$Z_{i}$与$Z_{j}$的相关系数为0，即：
$$
r_{z_{i},z_{j}}=\frac{Cov(Z_{i},Z_{j})}{\sqrt{Cov(Z_{i},Z_{j})Cov(Z_{j},Z{j})}}=0(i \neq j)
$$
因此各主成分间的相关系统矩阵为单位矩阵。

#### 2. 主成分的贡献率和累积贡献率
可以证明，各原始指标$X_{1},X_{2},\dots,X_{m}$的方差和与各主成分$Z_{1},Z_{2},\dots,Z_{m}$的方差和相等，将数据标准化后，原始指标的方差和为$\sum\limits_{i=1}^m \lambda_{i}$，即有$m=\sum\limits_{i=1}^m \lambda_{i}$

各指标所提供的信息量是用其方差来衡量的。由此可知，主成分分析是把m个原始指标$X_{1},X_{2},\dots,X_{m}$的总方差分解为m个互不相关的综合指标$Z_{1},Z_{2},\dots,Z_{m}$的方差之和，使第一主成分的方差达到最大，即变化最大的方向微量所相应的线性函数，最大方差为$\lambda$。其中${\lambda}/{\sum\limits_{i=1}^m \lambda_{i}}$表明了第一主成分$Z_{1}$的方差在全部方差中所占的比值，称为第一主成分的贡献率，这个值越大，表明$Z_{1}$这个指标综合原指标$X_{1},X_{2},\dots,X_{m}$的能力越强，也可以说，由$Z_{1}$的差异来解释$X_{1},X_{2},\dots,X_{m}$的差异的能力越强，正是因为这一点，才把$Z_{1}$称为$X_{1},X_{2},\dots,X_{m}$的第一主成分，也就是$X_{1},X_{2},\dots,X_{m}$的主要部分，了解到这一点，就可以知道为什么主成分是按特征值$\lambda_{1},\lambda_{2},\dots,\lambda_{m}$进行排序的。

通常情况下
$$
\frac{\lambda_{i}}{\sum\limits_{i=1}^m}=\frac{\lambda_{i}}{m}(k=1,2,\dots,m)
$$
为第i主成分的贡献率；而称
$$
\sum\limits_{i=1}^k \frac{\lambda_{i}}{m}(k \leq m)
$$
为前k个主成分的累积贡献率。

#### 3. 主成分个数的选取

通常并不需要全部的主成分，只用其中的前几个，一般来说，主成分的保留个数按以下的原则来进行：
##### (1)以累积贡献率来确定：

当前k个主成分的累积贡献率达到某一特定值时（一般以大于70%为宜），则保留前k个主成分。
##### (2)以特征值的大小来确定：

即若主成分$Z_{i}$的特征值$\lambda_{i} \geq 1$，则保留$Z_{i}$，否则就去掉该主成分。

#### 4. 因子载荷

为了了解各主成分与各原始指标之间的关系，在主成的表达式中，第$i$主成分$Z_{i}$的特征值的平方根$\sqrt{\lambda_{i}}$与第$j$原始指标$X_{j}$的系数$a_{ij}$的乘积，即
$$
q_{ij}=\sqrt{\lambda_{i}a_{ij}}
$$
为因子载荷（factor loading），由因子载荷构成的矩阵称为因子载荷阵，事实上因子载荷$q_{ij}$就是第$i$主成分$Z_{i}$与第$j$原始指标$X_{j}$之间的相关系数，它反映了主成分$Z_{i}$与原始指标X_{i}$之间联系的密切程度与作用的方向。

#### 5. 样品的主成得分
对于具有原始指标测定值$(X_{i1},X_{i2},\dots,X_{im})$的任一样品，可先用标准化变换式将原始数据标化，即：

$$
X'_{ij}=\frac{X_{ij}-\bar{X_{j}}}{S_{J}}(j=1,2,3,\dots,m)
$$

然后代入各主成分的表达式，即
$$
Z_{i}={\bf a'_{i}X}={a_{i1}X_{1}+a_{i2}X_{2}+ \dots +a_{im}X_{m}} i=1,2,\dots,m
$$
求出该样本各主成分值，这样求得的主成分值称为该样本的主成分得分，利用样品的主成分得分，可以对样品的特征进行推断和评价。













案例一：
例22-1  某研究者测得84名10岁男孩的身高、坐高、体重、胸围、肩宽、肺活量等6项生长发育指标，数据见表22-2。试作主成分分析。
![](http://ortxpw68f.bkt.clouddn.com/pca001.png)

用的包是psych，运算过程如下：

```
raw_pca <- read.csv("https://raw.githubusercontent.com/20170505a/raw_data/master/data_szq_2201.csv")
library(psych)
fa.parallel(raw_pca[-1],fa='pc',n.iter=100,show.legend = FALSE,main='Scree plot with parallel analysis')
```

图中展示了基于观测特征值的碎石检验（蓝色）、根据100个随机数据矩阵推导出来的特征值均值（红色），从图中可以看出：碎石检验图形最大变化处上面只有一个成分；特征值大于随机模拟数据的也只有一个主成分；特征值大于1的也只有一个主成分。所以这一组数据使用一个主成分即可保留数据集的大部分信息。








R基础安装包中的主成分分析函数：princomp()


探索性因子分析（EFA）是一系列用来发现一组变量的潜在结构的方法，通过寻找一组更小的、潜在的或隐藏的结构来解释已观测到的、显式的变量间的关系。因子被当做是观测变量的结构基础或“原因”，而不是它们的线性组合。因子间可能会有相关性。因子分析需要5~10倍于变量数的样本数。R基础安装包中的EFA函数：factanal()。


## 参考资料
1. 医学统计学.孙振球.第4版
2. [如何通俗易懂地解释「协方差」与「相关系数」的概念？](https://www.zhihu.com/question/20852004)
3. [浅谈协方差矩阵](http://pinkyjie.com/2010/08/31/covariance/)
4. [如何理解矩阵特征值和特征向量？](https://mp.weixin.qq.com/s/-FuNX0LiKHNRysSdgenK_g)




